Title: Reinforcement learningSummary: Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).
The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.Sections:   - Introduction: Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
Basic reinforcement learning is modeled as a Markov decision process (MDP):

a set of environment and agent states, S;
a set of actions, A, of the agent;

  
    
      
        
          P
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
        =
        Pr
        (
        
          s
          
            t
            +
            1
          
        
        =
        
          s
          ′
        
        ∣
        
          s
          
            t
          
        
        =
        s
        ,
        
          a
          
            t
          
        
        =
        a
        )
      
    
    {\displaystyle P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)}
   is the probability of transition (at time 
  
    
      
        t
      
    
    {\displaystyle t}
  ) from state 
  
    
      
        s
      
    
    {\displaystyle s}
   to state 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
   under action 
  
    
      
        a
      
    
    {\displaystyle a}
  .

  
    
      
        
          R
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    {\displaystyle R_{a}(s,s')}
   is the immediate reward after transition from 
  
    
      
        s
      
    
    {\displaystyle s}
   to 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
   with action 
  
    
      
        a
      
    
    {\displaystyle a}
  .The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the "reward function" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.A basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time t, the agent receives the current state 
  
    
      
        
          s
          
            t
          
        
      
    
    {\displaystyle s_{t}}
   and reward 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
  . It then chooses an action 
  
    
      
        
          a
          
            t
          
        
      
    
    {\displaystyle a_{t}}
   from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state 
  
    
      
        
          s
          
            t
            +
            1
          
        
      
    
    {\displaystyle s_{t+1}}
   and the reward 
  
    
      
        
          r
          
            t
            +
            1
          
        
      
    
    {\displaystyle r_{t+1}}
   associated with the transition 
  
    
      
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        ,
        
          s
          
            t
            +
            1
          
        
        )
      
    
    {\displaystyle (s_{t},a_{t},s_{t+1})}
   is determined. The goal of a reinforcement learning agent is to learn a policy: 
  
    
      
        π
        :
        A
        ×
        S
        →
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \pi :A\times S\rightarrow [0,1]}
  , 
  
    
      
        π
        (
        a
        ,
        s
        )
        =
        Pr
        (
        
          a
          
            t
          
        
        =
        a
        ∣
        
          s
          
            t
          
        
        =
        s
        )
      
    
    {\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}
   which maximizes the expected cumulative reward.
Formulating the problem as an MDP assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.
When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.
Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage operation, robot control, elevator scheduling, telecommunications, photovoltaic generators dispatch, backgammon, checkers and Go (AlphaGo).
Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:

A model of the environment is known, but an analytic solution is not available;
Only a simulation model of the environment is given (the subject of simulation-based optimization);
The only way to collect information about the environment is to interact with it.The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.   - Exploration: The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space MDPs in Burnetas and Katehakis (1997).Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite MDPs is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
One such method is 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -greedy, where 
  
    
      
        0
        <
        ε
        <
        1
      
    
    {\displaystyle 0<\varepsilon <1}
   is a parameter controlling the amount of exploration vs. exploitation.  With probability 
  
    
      
        1
        −
        ε
      
    
    {\displaystyle 1-\varepsilon }
  , exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  , exploration is chosen, and the action is chosen uniformly at random. 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.   - Algorithms for control learning: Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.   - Theory: Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
Efficient exploration of MDPs is given in  Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
For incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).   - Research: Research topics include: 

actor-critic
adaptive methods that work with fewer (or no) parameters under a large number of conditions
bug detection in software projects
continuous learning
combinations with logic-based frameworks
exploration in large MDPs
human feedback
interaction between implicit and explicit learning in skill acquisition
intrinsic motivation which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations
large (or continuous) action spaces
modular and hierarchical reinforcement learning
multiagent/distributed reinforcement learning is a topic of interest. Applications are expanding.
occupant-centric control
optimization of computing resources
partial information (e.g., using predictive state representation)
reward function based on maximising novel information
sample-based planning (e.g., based on Monte Carlo tree search).
securities trading
transfer learning
TD learning modeling dopamine-based learning in the brain. Dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error.
value-function and policy search methods   - Comparison of reinforcement learning algorithms:    - See also:    - References:    - Further reading: Annaswamy, Anuradha M. (3 May 2023). "Adaptive Control and Intersections with Reinforcement Learning". Annual Review of Control, Robotics, and Autonomous Systems. 6 (1): 65–93. doi:10.1146/annurev-control-062922-090153. ISSN 2573-5144. S2CID 255702873.
Auer, Peter; Jaksch, Thomas; Ortner, Ronald (2010). "Near-optimal regret bounds for reinforcement learning". Journal of Machine Learning Research. 11: 1563–1600.
Busoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010). Reinforcement Learning and Dynamic Programming using Function Approximators. Taylor & Francis CRC Press. ISBN 978-1-4398-2108-4.
François-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). "An Introduction to Deep Reinforcement Learning". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. Bibcode:2018arXiv181112560F. doi:10.1561/2200000071. S2CID 54434537.
Powell, Warren (2011). Approximate dynamic programming: solving the curses of dimensionality. Wiley-Interscience.
Sutton, Richard S.; Barto, Andrew G. (2018). Reinforcement Learning: An Introduction (2 ed.). MIT Press. ISBN 978-0-262-03924-6.
Sutton, Richard S. (1988). "Learning to predict by the method of temporal differences". Machine Learning. 3: 9–44. doi:10.1007/BF00115009.
Szita, Istvan; Szepesvari, Csaba (2010). "Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds" (PDF). ICML 2010. Omnipress. pp. 1031–1038. Archived from the original (PDF) on 2010-07-14.   - External links: Reinforcement Learning Repository
Reinforcement Learning and Artificial Intelligence (RLAI, Rich Sutton's lab at the University of Alberta)
Autonomous Learning Laboratory (ALL, Andrew Barto's lab at the University of Massachusetts Amherst)
Real-world reinforcement learning experiments Archived 2018-10-08 at the Wayback Machine at Delft University of Technology
Stanford University Andrew Ng Lecture on Reinforcement Learning
Dissecting Reinforcement Learning Series of blog post on RL with Python code
A (Long) Peek into Reinforcement Learning